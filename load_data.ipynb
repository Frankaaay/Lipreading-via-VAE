{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6df2be89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in c:\\anaconda\\envs\\directml\\lib\\site-packages (0.3.12)\n",
      "Requirement already satisfied: packaging in c:\\anaconda\\envs\\directml\\lib\\site-packages (from kagglehub) (25.0)\n",
      "Requirement already satisfied: pyyaml in c:\\anaconda\\envs\\directml\\lib\\site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\envs\\directml\\lib\\site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\envs\\directml\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\envs\\directml\\lib\\site-packages (from requests->kagglehub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\envs\\directml\\lib\\site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda\\envs\\directml\\lib\\site-packages (from requests->kagglehub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\envs\\directml\\lib\\site-packages (from requests->kagglehub) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\envs\\directml\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\anaconda\\envs\\directml\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\anaconda\\envs\\directml\\lib\\site-packages (from scikit-learn) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\anaconda\\envs\\directml\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\anaconda\\envs\\directml\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\anaconda\\envs\\directml\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\anaconda\\envs\\directml\\lib\\site-packages (from opencv-python) (2.2.5)\n",
      "Downloading opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl (39.5 MB)\n",
      "   ---------------------------------------- 0.0/39.5 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 9.7/39.5 MB 46.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 22.5/39.5 MB 52.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 32.2/39.5 MB 51.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.8/39.5 MB 47.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.5/39.5 MB 40.5 MB/s eta 0:00:00\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.11.0.86\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub\n",
    "!pip install scikit-learn\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e67f423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/mohamedbentalb/lipreading-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404M/404M [00:08<00:00, 51.4MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\6\\.cache\\kagglehub\\datasets\\mohamedbentalb\\lipreading-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mohamedbentalb/lipreading-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15c7f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch_directml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43292d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 video-align pairs.\n",
      "Training set size: 800\n",
      "Test set size: 200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "video_dir = r\"mohamedbentalb\\lipreading-dataset\\versions\\1\\data\\s1\"\n",
    "align_dir = r\"mohamedbentalb\\lipreading-dataset\\versions\\1\\data\\alignments\\s1\"\n",
    "\n",
    "# Pair each video with its alignment file\n",
    "video_files = [f for f in os.listdir(video_dir) if f.endswith('.mpg')]\n",
    "video_align_pairs = []\n",
    "for vf in video_files:\n",
    "    base = os.path.splitext(vf)[0]\n",
    "    align_path = os.path.join(align_dir, base + \".align\")\n",
    "    video_path = os.path.join(video_dir, vf)\n",
    "    if os.path.exists(align_path):\n",
    "        video_align_pairs.append((video_path, align_path))\n",
    "\n",
    "print(f\"Found {len(video_align_pairs)} video-align pairs.\")\n",
    "\n",
    "train_pairs, test_pairs = train_test_split(video_align_pairs, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_pairs)}\")\n",
    "print(f\"Test set size: {len(test_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "caf80ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing transform\n",
    "transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((112, 112)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.43216, 0.394666, 0.37645], [0.22803, 0.22145, 0.216989])\n",
    "])\n",
    "\n",
    "def preprocess_frames(frames):\n",
    "    processed = [transform(frame) for frame in frames]\n",
    "    video_tensor = torch.stack(processed)  # (T, C, H, W)\n",
    "    video_tensor = video_tensor.permute(1, 0, 2, 3)  # (C, T, H, W)\n",
    "    return video_tensor.unsqueeze(0)  # (1, C, T, H, W)\n",
    "\n",
    "def parse_align_file(align_path):\n",
    "    alignments = []\n",
    "    with open(align_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 3:\n",
    "                start, end, word = parts\n",
    "                alignments.append((int(start), int(end), word))\n",
    "    return alignments\n",
    "\n",
    "def extract_word_frames(video_path, alignments, fps=25):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    cap.release()\n",
    "\n",
    "    word_frames = []\n",
    "    for start, end, word in alignments:\n",
    "        start_idx = int(start / 1000 * fps)\n",
    "        end_idx = int(end / 1000 * fps)\n",
    "        word_seq = frames[start_idx:end_idx]\n",
    "        if word_seq:\n",
    "            word_frames.append((word, word_seq))\n",
    "    return word_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7bf60333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 16, 112, 112])\n",
      "Word: sil, Feature shape: torch.Size([1, 400])\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch_directml\n",
    "import numpy as np\n",
    "\n",
    "# device = torch_directml.device()\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "video_path, align_path = train_pairs[1]\n",
    "alignments = parse_align_file(align_path)\n",
    "word_frames = extract_word_frames(video_path, alignments, fps=25)\n",
    "\n",
    "# Load model\n",
    "model = torchvision.models.video.r3d_18(pretrained=True)\n",
    "# model = torchvision.models.video.r2plus1d_18(pretrained=True)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "MIN_FRAMES = 16\n",
    "\n",
    "with torch.no_grad():\n",
    "    for word, frames in word_frames:\n",
    "        if len(frames) < MIN_FRAMES:\n",
    "            continue\n",
    "        # Sample exactly 16 frames evenly\n",
    "        if len(frames) > MIN_FRAMES:\n",
    "            idxs = np.linspace(0, len(frames)-1, MIN_FRAMES).astype(int)\n",
    "            frames = [frames[i] for i in idxs]\n",
    "        video_tensor = preprocess_frames(frames).to(device)\n",
    "        print(video_tensor.shape)\n",
    "        features = model(video_tensor)\n",
    "        print(f\"Word: {word}, Feature shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67deee59",
   "metadata": {},
   "source": [
    "Here is another pretrained model: here is the instruction to pip install:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/astorfi/lipnet.git\n",
    "cd lipnet\n",
    "pip install -r requirements.txt\n",
    "pip install .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You must have LipNet installed and its modules accessible in your path\n",
    "from lipnet.model import LipNet\n",
    "\n",
    "\n",
    "# Load LipNet model (set decoder to None to use only encoder)\n",
    "model = LipNet(img_c=3, img_w=112, img_h=112, absolute_max_string_len=32, output_size=28)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "MIN_FRAMES = 75  # LipNet expects 75 frames per input\n",
    "\n",
    "with torch.no_grad():\n",
    "    for word, frames in word_frames:\n",
    "        if len(frames) < MIN_FRAMES:\n",
    "            continue\n",
    "        # Sample exactly 75 frames evenly\n",
    "        if len(frames) > MIN_FRAMES:\n",
    "            idxs = np.linspace(0, len(frames)-1, MIN_FRAMES).astype(int)\n",
    "            frames = [frames[i] for i in idxs]\n",
    "        video_tensor = preprocess_frames(frames).to(dml)\n",
    "        # Forward pass through encoder (LipNet's forward returns logits, you may want encoder features)\n",
    "        features = model.encoder(video_tensor)\n",
    "        print(f\"Word: {word}, Feature shape: {features.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "directml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
